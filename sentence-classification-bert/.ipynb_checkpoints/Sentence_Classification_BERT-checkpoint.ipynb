{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Sentence Classification with BERT\n", "This notebook demonstrates a full pipeline for classifying policy-related sentences using BERT.\n", "- We use `transformers` and `datasets` libraries.\n", "- The task is to classify sentences into 4 categories: `tax`, `subsidy`, `ban`, `other`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -U transformers datasets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer\n", "from datasets import Dataset\n", "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example labeled data\n", "data = {\n", "    'sentence': [\n", "        'The government introduced a new tax.',\n", "        'Grants were given to solar startups.',\n", "        'Plastic bags were banned in all cities.',\n", "        'Public awareness campaigns were launched.'\n", "    ],\n", "    'label': ['tax', 'subsidy', 'ban', 'other']\n", "}\n", "df = pd.DataFrame(data)\n", "label2id = {'tax': 0, 'subsidy': 1, 'ban': 2, 'other': 3}\n", "id2label = {v: k for k, v in label2id.items()}\n", "df['label'] = df['label'].map(label2id)\n", "dataset = Dataset.from_pandas(df)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tokenize the dataset\n", "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n", "def tokenize(batch):\n", "    return tokenizer(batch['sentence'], padding=True, truncation=True)\n", "dataset = dataset.map(tokenize, batched=True)\n", "dataset = dataset.train_test_split(test_size=0.25)\n", "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load the model\n", "model = BertForSequenceClassification.from_pretrained(\n", "    'bert-base-uncased',\n", "    num_labels=4,\n", "    id2label=id2label,\n", "    label2id=label2id\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TrainingArguments and compute_metrics\n", "training_args = TrainingArguments(\n", "    output_dir='./results',\n", "    num_train_epochs=3,\n", "    per_device_train_batch_size=4,\n", "    per_device_eval_batch_size=4,\n", "    evaluation_strategy=\"epoch\",\n", "    save_strategy=\"epoch\",\n", "    logging_dir='./logs',\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model='accuracy'\n", ")\n", "def compute_metrics(p):\n", "    preds = p.predictions.argmax(-1)\n", "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n", "    acc = accuracy_score(p.label_ids, preds)\n", "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the model\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=dataset['train'],\n", "    eval_dataset=dataset['test'],\n", "    compute_metrics=compute_metrics,\n", "    tokenizer=tokenizer\n", ")\n", "trainer.train()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": ""}}, "nbformat": 4, "nbformat_minor": 2}